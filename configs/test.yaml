experiment:
  name: test-v2
  description: test-v2
  save_log: False
  save_trajectory: False
  save_checkpoints: False
  save_interval: 1000
  progress_interval: 50
  cpu_num: 2
  iteration_max: 500

environment:
  id: pytorch-predictive-agent/MovingImageEnvironment-v0
  render_mode: none
  agent_speed: 10
  noise_scale: 3.0

load_path:
  feature_extractor: False
  inverse_network: False
  inner_state_predictor: False
  feature_predictor: False
  controller: False

hyperparameter:
  batch_size: 128
  random_policy: False
  optimizer: sgd
  gradient_clipping: 100
  learning_rate: 1.0e-4
  inverse_loss_scale: 0.8
  predictor_loss_scale: 0.2
  value_loss_scale: 0.8
  policy_loss_scale: 0.2
  entropy_scale: 0.0
  gamma: 0.99
  lmbda: 0.95
  intrinsic_reward_scale: 0.5

network_spec:
  feature_extractor:
    initialization: True
    layers:
      # 3 x 64 x 64
      - layer: conv2d 
        spec: [3, 32, 3, 2, 1]
        activation: elu
        # 32 x 32 x 32
      - layer: conv2d
        spec: [32, 32, 3, 2, 1]
        activation: elu
        # 32 x 16 x 16
      - layer: conv2d
        spec: [32, 32, 3, 2, 1]
        activation: elu
        # 32 x 8 x 8 
      - layer: conv2d
        spec: [32, 16, 3, 2, 1]
        activation: elu
        # 16 x 4 x 4 (= 256)
      - layer: flatten

  inverse_network:
    initialization: True
    layers:
      - layer: linear
        spec: [512, 256]
        activation: relu
      - layer: linear
        spec: [256, 4]
        activation: softmax

  inner_state_predictor:
    initialization: True
    layers:
      - layer: gru
        spec: [260, 256, 1] # 4 + 256
        activation: elu

  feature_predictor:
    initialization: True
    layers:
      - layer: linear
        spec: [260, 256] # 4 + 256
        activation: elu
      - layer: linear
        spec: [256, 256]
        activation: elu

  controller_shared:
    initialization: True
    layers:
      - layer: linear
        spec: [256, 128]
        activation: relu
      - layer: linear
        spec: [128, 64]
        activation: relu
  controller_actor:
    initialization: True
    layers:
      - layer: linear
        spec: [64, 64]
        activation: relu
      - layer: linear
        spec: [64, 4]
        activation: softmax
  controller_critic:
    initialization: True
    layers:
      - layer: linear
        spec: [64, 64]
        activation: relu
      - layer: linear
        spec: [64, 1]
        activation: False